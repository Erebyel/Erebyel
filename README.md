Erebyel es mi pseudónimo/marca; en el mundo real suelen llamarme Reyes. Me considero una mente inquieta a la que le gusta escribir, dibujar, leer, la ciencia y la tecnología. Resulta que, después de años evitando programar por *traumas infantiles* con un Win95, es algo que me gusta y en lo que estoy actualmente profundizando; por suerte, el pensamiento abstracto, lateral y computacional ya los llevaba años trabajando; además me gusta arreglar cosas y buscar soluciones.

## ¿Qué estoy haciendo actualmente?
En mi plan inicial, quería dedicar los últimos meses de 2020 y el inicio de 2021 para ir recopilando y poniendo bonito mi porfolio de Ciencia de Datos; rara vez las cosas ocurren en el tiempo en el que lo planeamos. Eso de que *la vida tiene otros planes* y he tenido que posponerlo para la segunda mitad de 2021.

Poco después de finalizar el *bootcamp* de **Ciencia de datos** en The Bridge (finales de octubre de 2020), me ofrecieron ser la **Profesora Auxiliar de Ciencia de Datos** para la siguiente promoción que impartiría José Ramón (un profesor maravilloso); por supuesto, acepté porque la experiencia me ayudaría tanto a reforzar como a fijar los conocimientos que ya había adquirido.

Por supuesto, mantengo mis otras actividades habituales: soy autónoma en **edición y corrección de textos**, además de hacer algún trabajo de tratamiento de datos. En cuanto a la edición, aunque no es algo que haya buscado, me he especializado en la literatura médica, manuales y contratos.

Por ahora, en www.erebyel.es, hay algunos contenidos relacionados con la programación (muy pocos). En algún momento, convertiré este GitHub en mi porfolio y dejaré la web para el resto: escribir, locutar, dibujar y que me conozcáis mejor.

Además, como se puede predecir, me quiero especializar en **Procesamiento de Lenguaje Natural** y actualmente estoy realizando un par de cursos de **TensorFlow** y **Keras** avanzado. Mi intención es seguir después aprendiendo más sobre **Transformers**, **Bert** y lo que esté por venir. Además de algunas herramientas como **Rasa** y la creación de asistentes virtuales *chatbots*.

## ¿Qué soy capaz de hacer?
Después del curso inmersivo me siento cómoda trabajando con:
- **Python** (reconozco que las clases todavía se me resisten un poco).
- **RegEx**: trabajo de expresiones regulares con Python.
- **Sistemas de control de versiones**: Git + GitHub.
- **Bases de datos** (con las que ya trato desde la carrera (**Grado en Información y Documentación**):
    - SQL + Diagramas UML y Reglas Cood.
    - MongoDB (no relacional con .json).
    - Neo4j (Cypher) (grafos).
    - Tengo pendiente de cotillear eso que llaman *NewSQL*.

- **Búsqueda y extracción de datos** con la configuración y uso de APIs, técnicas de *Webscraping* con Selenium.

- **Análisis de datos**: descriptivos, exploratorios y predictivos trabajando con las herramientas que ofrecen las bibliotecas de Python: **Pandas** y **NumPy**…
    - Codificación de archivos.
    - Tratamiento de datos perdidos, fuera de rango, raros, etiquetas, fechas, inconsistentes, conjuntos…
    - Codificación de variables (*dummies* y *One Hot/Cold Encoding*).
- y **Visualización y narrativa de datos (*storytelling*)**, con **MatPlotLib**, **Seaborn** y **Plotly**.

- **Ingeniería de características**
    - Escalado y normalización.
    - Corrección de distribuciones.
    - Selección de características.
    - Creación de características.
    - Reducción de la dimensionalidad.

- **Aprendizaje automático**, modelos del aprendizaje supervisado y no supervisado para aplicar realizar convenientemente predicciones.
    - El *descenso de gradiente*.
    - Scikit-learn (**sklearn**, para los amigos).
    - Funciones.
    - Herramientas para el entrenamiento, métricas y prediccion de modelos:
        - Elección del modelo: Validación Cruzada (*crossvalidation*).
        - Ajuste de hiperparámetros: GridSearch o RandomSearch para buscar los mejores parámetros para el modelo.
        - Optimización de hiperparámetros.
    - Algoritmos genéticos.
    - Detección de anomalías.
    - Trabajo con Series Temporales.
        - Statsmodels.
        - Introducción a los modelos: AR, MA, ARMA, ARIMA, SARIMA y SARIMAX.

- **Redes Neuronales**, introducción a los tipos de redes neuronales que existen, a TensorFlow y Keras. *Actualmente estoy profundizando en esto.*
    - Tipos de redes neuronales:
        - Convolucionales.
        - Recurrentes (LSTM).
        - Generativas Adversarias.
    - Word2Vec, Glove, Attention.
    - OpenCV.
    - Aumentación de datos.
    - Procesamiento de Lenguaje Natural con NLTK y Spacy.
        - Modelos del lenguaje NLU y NLG.
        - Herramientas y estado del arte
        - Tranformers.
        - Autoencoders.

- **Big Data con Pyspark**
    -  Uso de Hadoop, Spark y PySpark
    -  SparkML

- **Producción**, donde nos introdujeron en:
    - Entornos de pruebas
    - Creación de entornos virtuales
    - Flask
    - Docker
    - Kubernetes
    - Google Cloud

**¿Por qué he puesto este currículo?**
Una de las formas que tengo para reforzar y profundizar lo que aprendo es escribir explicaciones con mis palabras; lo que vienen a ser apuntes. Mi intención es ir corrigiendo y preparándolos correctamente para compartirlos y que formen parte de mi porfolio. Serán contenidos y explicaciones íntegramente en castellano; por lo que, además, podrían ayudar a quien esté comenzando a aprender sobre estos temas.
Por supuesto, también incluiré algunas prácticas y ejercicios que he ido haciendo y lo que vaya descubriendo.

Sobre **Gilbert**: aunque eso ya lo tenía en mente desde el principio, tengo pendiente darle una vuelta, ya que quería aprovechar que contiene errores para hablar sobre las Pep8 y algunas cosillas más relacionada con la creación de clases en Python, espero poder hacerlo pronto.
