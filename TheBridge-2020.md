## ¿Qué aprendí en The Bridge?
El curso fue muy intenso y dimos muchas pinceladas a muchos temas; por supuesto, no tengo intención de profundizar en todos. Tengo claro porqué quería meterme y cuál es la ruta que quiero seguir. Tengo la intención, al menos con las partes que me resultan más interesantes, ir generando el repositorio de [Ciencia de datos](https://github.com/Erebyel/ciencia-de-datos). No estará todo, porque hay cosas en las que a estas alturas no quiero detenerme.

De la lista que expongo a continuación, tengo al menos conocimientos básicos y, aúnque no sean herramientas que utilice a diario, puedo ponerme a día rápido. Además, tengo la ventaja de que, pese a pasar de los 30, sigo teniendo el cerebro bastante flexible y ¡aprendo rápido!

- **Python** (reconozco que las clases todavía se me resisten un poco).
- **RegEx**: trabajo de expresiones regulares con Python.
- **Sistemas de control de versiones**: Git + GitHub.
- **Bases de datos** (con las que ya trato desde la carrera (**Grado en Información y Documentación**):
    - SQL + Diagramas UML y Reglas Cood.
    - MongoDB (no relacional con .json).
    - Neo4j (Cypher) (grafos).
    - Tengo pendiente de cotillear eso que llaman *NewSQL*.

- **Búsqueda y extracción de datos** con la configuración y uso de APIs, técnicas de *Webscraping* con Selenium.

- **Análisis de datos**: descriptivos, exploratorios y predictivos trabajando con las herramientas que ofrecen las bibliotecas de Python: **Pandas** y **NumPy**…
    - Codificación de archivos.
    - Tratamiento de datos perdidos, fuera de rango, raros, etiquetas, fechas, inconsistentes, conjuntos…
    - Codificación de variables (*dummies* y *One Hot/Cold Encoding*).
- y **Visualización y narrativa de datos (*storytelling*)**, con **MatPlotLib**, **Seaborn** y **Plotly**.

- **Ingeniería de características**
    - Escalado y normalización.
    - Corrección de distribuciones.
    - Selección de características.
    - Creación de características.
    - Reducción de la dimensionalidad.

- **Aprendizaje automático**, modelos del aprendizaje supervisado y no supervisado para aplicar realizar convenientemente predicciones.
    - El *descenso de gradiente*.
    - Scikit-learn (**sklearn**, para los amigos).
    - Funciones.
    - Herramientas para el entrenamiento, métricas y prediccion de modelos:
        - Elección del modelo: Validación Cruzada (*crossvalidation*).
        - Ajuste de hiperparámetros: GridSearch o RandomSearch para buscar los mejores parámetros para el modelo.
        - Optimización de hiperparámetros.
    - Algoritmos genéticos.
    - Detección de anomalías.
    - Trabajo con Series Temporales.
        - Statsmodels.
        - Introducción a los modelos: AR, MA, ARMA, ARIMA, SARIMA y SARIMAX.

- **Redes Neuronales**, introducción a los tipos de redes neuronales que existen, a TensorFlow y Keras. *Actualmente estoy profundizando en esto.*
    - Tipos de redes neuronales:
        - Convolucionales.
        - Recurrentes (LSTM).
        - Generativas Adversarias.
    - Word2Vec, Glove, Attention.
    - OpenCV.
    - Aumentación de datos.
    - Procesamiento de Lenguaje Natural con NLTK y Spacy.
        - Modelos del lenguaje NLU y NLG.
        - Herramientas y estado del arte
        - Tranformers.
        - Autoencoders.

- **Big Data con Pyspark**
    -  Uso de Hadoop, Spark y PySpark
    -  SparkML

- **Producción**, donde nos introdujeron en:
    - Entornos de pruebas
    - Creación de máquinas virtuales
    - Flask
    - Docker
    - Kubernetes
    - Google Cloud
